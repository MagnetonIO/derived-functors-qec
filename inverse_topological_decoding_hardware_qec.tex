\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{fullpage}
\usepackage{physics}  % for \ket, \bra, etc (if needed)
\usepackage{graphicx}
\usepackage{hyperref}

\begin{document}

\title{Inverse Topological Decoding and Derived Hamiltonians \\  A Hardware-Embedded Approach to Quantum Error Correction}
\author{Matthew Long \\
Magneton Labs}
\date{\today}

\maketitle

\begin{abstract}
Topological quantum error-correcting codes have emerged as a leading approach to protecting quantum information from decoherence. Traditionally, error \emph{decoding} in such codes is performed via classical algorithms that interpret syndrome measurements. Here we outline the concept of \textbf{Inverse Topological Decoding} at the hardware level, an approach that \emph{inverts} the usual decoding process by embedding error-correction directly into the physical dynamics of the quantum hardware. We provide a detailed theoretical framework for this approach, including a mathematical formalism based on operator algebras, category theory, and homotopy type theory, and we formally define \emph{Derived Hamiltonians} that drive inverse decoding. We then discuss the real physics of implementing inverse topological decoding on various platforms---from superconducting qubits and tunable coupler circuits to Majorana-based topological qubits and trapped ion systems. Engineering considerations such as cryogenics, materials, and fabrication challenges are analyzed. We compare the computational complexity and fault-tolerance implications of this hardware-embedded decoding to conventional quantum error correction, showing potential advantages in scalability. Finally, we propose practical integration strategies and outline future research directions, aiming to bridge advanced theoretical concepts with experimental quantum computing architectures.
\end{abstract}

\section{Introduction}
Quantum error correction is essential for scalable quantum computing, as physical qubits are inherently prone to errors from decoherence, noise, and control imperfections. Topological quantum error-correcting codes, such as the surface code and color codes, are particularly promising due to their high error threshold and the fact that logical information is stored in global (topological) degrees of freedom that are less sensitive to local noise \cite{Dennis2002, Fowler2012}. In a typical topological code implementation, error correction proceeds by measuring local check operators (stabilizers), and then using a classical decoding algorithm to infer and correct errors. This process can be viewed as a pipeline: physical error $\to$ syndrome measurement $\to$ classical decoding $\to$ correction operation. 

In this paper, we explore an alternative paradigm termed \emph{Inverse Topological Decoding}. Instead of relying on an external classical decoder to determine corrections, the idea is to encode the decoding process \emph{into the quantum hardware itself}. In other words, we seek to engineer physical systems (Hamiltonians, interactions, and possibly dissipative dynamics) such that the occurrence of errors triggers an automatic, hardware-level response that ``decodes'' and counteracts those errors. This approach inverts the usual scheme: the hardware applies the inverse of likely error operators as part of its natural dynamics, maintaining the encoded logical state without requiring explicit measurement and software intervention.

The concept of inverse decoding at the hardware level raises deep theoretical and practical questions:
\begin{itemize}
    \item How can we formally describe a quantum system that inherently stabilizes a logical subspace against local errors? What advanced mathematical tools (operator algebras, category theory, homotopy) are useful for modeling such a system?
    \item What is the \textit{Derived Hamiltonian} that generates this autonomous error-correcting dynamics? How is it constructed from the structure of a topological code?
    \item Can such Hamiltonians (or equivalent dissipative processes) be realized with current or near-future quantum hardware platforms like superconducting circuits, Majorana zero-mode devices, or trapped ions? What are the underlying physical mechanisms?
    \item What challenges arise in engineering and fabricating a quantum processor that follows inverse topological decoding principles (e.g., complexity of multi-qubit interactions, cryogenics, materials)? 
    \item How does this approach impact computational complexity and fault-tolerance thresholds compared to conventional quantum error correction (QEC)? Could it reduce overhead and improve scalability?
\end{itemize}

We address these questions in a structured manner. In Section~2, we develop the mathematical formalism for inverse topological decoding, introducing operator algebra descriptions of codes, categorical frameworks for fault-tolerant logical information, and homotopy-theoretic interpretations of error classes. We define the notion of Derived Hamiltonians that naturally emerge from code stabilizers and discuss higher-category structures that classify logical operators and error processes. Section~3 delves into the physics of implementing these ideas on hardware. We examine how various quantum computing platforms can realize the required Hamiltonians or interactions --- for example, how a network of superconducting qubits with tunable couplers could emulate a topologically ordered phase whose ground states encode logical qubits, or how Majorana fermion systems intrinsically realize topologically protected operations. We also discuss the experimental feasibility of these implementations and survey progress in creating setups that could demonstrate autonomous error correction. Section~4 focuses on engineering considerations: whether it is feasible to fabricate large-scale processors with these features, what cryogenic and material constraints appear, and how to overcome potential fabrication and integration challenges. In Section~5, we analyze computational and theoretical implications. We compare the complexity of error suppression via a physical Hamiltonian to classical decoding algorithms, examine the expected fault-tolerance thresholds and overhead, and consider the implications for quantum computer architecture and scalability. Section~6 (Practical Implementations and Outlook) synthesizes these insights, proposing how inverse topological decoding could be integrated into near-term quantum computing architectures, how one might experimentally validate the theory (for instance, through small-scale prototypes or specific syndrome dynamics measurements), and which future research directions are most critical to advance this field. We conclude that inverse topological decoding, while challenging, offers a tantalizing route toward more resilient quantum hardware by merging error correction with physical law.

\section{Mathematical Formalism}
In this section, we establish a rigorous framework for inverse topological decoding using a range of advanced mathematical tools. We start by reviewing the operator algebra formulation of stabilizer codes and then introduce category-theoretic perspectives that help conceptualize logical information in a topologically robust manner. We then propose a formal definition of \emph{Derived Hamiltonians} for inverse decoding and discuss how higher-category structures (such as $2$-categories or beyond) and homotopy type theory can be applied to classify and ensure fault-tolerant logical state encoding.

\subsection{Operator Algebra and Stabilizer Formalism}
Topological quantum codes are often expressed in terms of operator algebras acting on many-qubit Hilbert spaces. The most common family, CSS (Calderbank-Shor-Steane) codes, including the surface code, can be described by a set of commuting Pauli operators (stabilizers) $S_i$ that define a code subspace \cite{Gottesman1997}. For a stabilizer code, the code space $\mathcal{H}_{\text{code}}$ is the joint $+1$ eigenspace of all stabilizer operators:
\[ 
\mathcal{H}_{\text{code}} = \{ \ket{\psi} : S_i \ket{\psi} = +\ket{\psi} \text{ for all } i \}. 
\]
These $S_i$ generate an abelian subgroup $\mathcal{S}$ of the $n$-qubit Pauli group (excluding $-I$ to avoid trivial degeneracy). Logical operators $\bar{X}, \bar{Z}, \dots$ are Pauli (or more general unitary) operators that commute with all elements of $\mathcal{S}$ but are not themselves in $\mathcal{S}$; they act non-trivially on the code space and distinguish the different logical states. In topological codes, stabilizers are usually local (acting on a small number of neighboring qubits on a lattice), whereas logical operators are often \emph{global} strings or loops of Pauli operators that stretch across the code region.

A key property of topological codes is that errors (local Pauli errors on physical qubits) have a geometrical interpretation. For example, in the surface code \cite{Kitaev2003, Dennis2002}, bit-flip type errors ($X$ errors) on qubits create pairs of syndrome excitations at the endpoints of the error chain. These excitations can be viewed as emergent quasiparticles (often called anyons) carrying $\mathbb{Z}_2$ charge. The syndrome measurement in an active error correction scheme identifies the locations of these anyons, and a decoding algorithm attempts to pair them up and annihilate them by applying corrective $X$ operations connecting the pairs. In operator language, a chain of physical $X$ errors anticommutes with those stabilizers at its endpoints, flipping their eigenvalues to $-1$ (these are the syndrome signals), while commuting with all stabilizers elsewhere. Two different error chains that have the same endpoints differ by a closed loop of $X$ operators which is itself a product of stabilizers; such a closed loop has no syndrome and is an example of a \emph{harmless} error (it lies in $\mathcal{S}$ and thus acts trivially on the code space). Only the homology class of the error chain (i.e., which boundary or loop it connects) is relevant for whether the error causes a logical fault.

In the Heisenberg picture, the effect of a physical error $E$ on a logical state can be understood by how $E$ acts on the operator algebra. If $E$ anticommutes with a stabilizer $S_i$, then $E S_i E^\dagger = -S_i$, indicating that the syndrome for $S_i$ flips sign. If instead $E$ commutes with all $S_i$ but anticommutes with a logical operator $\bar{L}$, then $E$ acts as a logical error on the code space (flipping a logical qubit). A cornerstone of fault-tolerance is that any two error operators $E_1, E_2$ that differ by a stabilizer (i.e., $E_1 = E_2 S$ for some $S \in \mathcal{S}$) have identical effect on the code space; they are operationally equivalent errors. In algebraic terms, the set of errors is partitioned into equivalence classes by the normal subgroup $\mathcal{S}$ of the Pauli group, and these equivalence classes correspond to elements of the quotient group (isomorphic to the group of logical operators). This algebraic structure is often leveraged for decoding: the decoder's goal is to determine which equivalence class an error belongs to, given the stabilizer measurement outcomes.

For inverse topological decoding, our aim is to integrate this idea of error equivalence directly into the hardware's physics. The hardware should be governed by interactions that penalize errors that would lead to a nontrivial equivalence class (i.e., a logical error), effectively pushing the system back toward the stabilizer subspace. This naturally leads to considering a Hamiltonian constructed from the stabilizer operators themselves. We formalize this next.

\subsection{Derived Hamiltonians for Autonomous Decoding}
We define a \textbf{Derived Hamiltonian} $H_{\mathrm{inv}}$ associated with a topological code as a Hamiltonian whose ground space coincides with the code space, and whose excitation structure corresponds to syndromes of the code. In its simplest form, one can consider:
\begin{equation} \label{eq:Ham}
    H_{\mathrm{inv}} \;=\; - \sum_{i} \Delta_i \, S_i \,,
\end{equation}
where $S_i$ are the stabilizer generators of the code and $\Delta_i > 0$ are energy coefficients (energy penalties for violating each stabilizer). In this Hamiltonian, any state that is an eigenstate of all $S_i$ with eigenvalue $+1$ (i.e., a code state) has the minimal energy $E_{\min} = -\sum_i \Delta_i$. An error that flips the value of a subset of stabilizers causes a corresponding increase in energy proportional to the number of stabilizers violated. For instance, a single-qubit $X$ error in the surface code will anticommute with (and thus violate) exactly two stabilizers (the two plaquettes adjacent to that edge qubit), creating two $-1$ syndrome outcomes which correspond to two anyon excitations of energy $2\Delta$ (here we assume uniform $\Delta_i=\Delta$ for simplicity).

Crucially, in $H_{\mathrm{inv}}$, errors are \emph{energetically suppressed} because they cost energy to create syndrome excitations. If the system is coupled to a cold bath or if it has some internal mechanism for relaxation, it will tend to remove these excitations: two anyons (syndrome excitations) can move together and annihilate, returning the stabilizers to $+1$ eigenvalue and reducing the energy by $2\Delta$. In an ideal scenario at zero temperature, $H_{\mathrm{inv}}$ has a degenerate ground space (the logical qubit subspace) and a finite energy gap to the first excited states (single anyon-pair excitations). If the energy gap is large compared to the environmental temperature $k_B T$ and other noise scales, the system will spend most of its time in the ground space, thus passively preserving the logical qubit. In this way, the Hamiltonian \eqref{eq:Ham} performs \emph{autonomous error correction}: any local error that causes an excitation can be viewed as triggering a physical process (e.g., two anyons attracting and annihilating) that is essentially the inverse of the error. This is the essence of inverse topological decoding at the Hamiltonian level.

We note, however, that the simple Hamiltonian of Eq.~\eqref{eq:Ham} is an idealization. While it captures the essential idea of a derived Hamiltonian, practical realizations may require additional terms or modifications:
\begin{itemize}
    \item \textbf{Higher-order interactions:} The stabilizer terms $S_i$ might be multi-qubit operators (e.g., products of 4 Pauli matrices on a plaquette in the surface code). Realizing such a term as a physical interaction could require gadget constructions (perturbative or multi-step coupling) which introduce effective higher-order terms or ancilla-mediated couplings. The derived Hamiltonian may thus involve auxiliary qubit degrees of freedom or multi-step processes to effectively generate the stabilizer interactions.
    \item \textbf{Perturbation and Stability:} In the presence of perturbations (e.g., disorder in coupling strengths, or additional non-commuting terms from hardware crosstalk), ensuring that the code space remains the ground space requires the perturbation to be sufficiently weak (compared to the gap) so as not to close the gap or favor a different ground state. The stability of the topologically ordered ground state under perturbations is guaranteed by certain conditions (known as the spectral or topological quantum order conditions \cite{Bravyi2010}), which our derived Hamiltonian should satisfy. We may need to add explicit \emph{boundary stabilizers} or pinning fields to define the code boundaries and prevent degeneracy splitting for open surfaces.
    \item \textbf{Dissipation vs. Hamiltonian dynamics:} In a purely Hamiltonian system at zero temperature, an error creates a pair of anyons that will remain indefinitely unless a dynamics or coupling to environment allows them to move and annihilate. A Hamiltonian alone that is static will conserve anyonic excitation number in many cases (since $[H_{\mathrm{inv}}, S_i] = 0$ for all $S_i$, syndrome excitations are good quantum numbers). Therefore, to actually achieve error correction (annihilation of anyons), one may rely on either quantum tunneling processes included in the Hamiltonian (small terms that cause hopping of excitations) or engineered coupling to a dissipative bath that can remove energy. A common strategy discussed in literature is \emph{engineered dissipation} that pumps the system to the code space by preferentially removing syndromic excitations \cite{Pastawski2009, Kapit2016}. In our formalism, one could augment $H_{\mathrm{inv}}$ with Lindblad operators in a master equation setting, but strictly speaking, the term "Hamiltonian" would then be part of a larger open-system framework.
\end{itemize}

The formal definition we propose is: \emph{A Derived Hamiltonian for inverse topological decoding is a Hamiltonian (possibly accompanied by controlled dissipation) constructed from the stabilizer operators of a quantum code such that (1) the code space is the ground eigenspace, (2) local errors correspond to localized excitations with an energy penalty, and (3) the system's natural dynamics causes these excitations to propagate and annihilate, effectively implementing error correction without external intervention.}

This definition extends beyond the simple Eq.~\eqref{eq:Ham} in that it allows additional terms that mobilize or couple excitations (e.g., hopping terms for anyons, or ancillary degrees of freedom that facilitate syndrome resolution). In a sense, $H_{\mathrm{inv}}$ encapsulates the entire error correction process: the syndrome \emph{detection} is encoded in the energy penalty, and the \emph{decoding} (pairing of anyons) is encoded in how excitations can move and lower energy.

From an operator algebra perspective, $H_{\mathrm{inv}}$ is diagonal in the stabilizer basis (commuting with all $S_i$), so it does not cause transitions between different syndrome sectors by itself. If we introduce a slight non-commuting term, say a small term $K$ that does not commute with each $S_i$ individually but respects the overall ground space, it could induce transitions like moving an anyon from one site to a neighboring site (hopping an excitation). One could envision designing $K$ such that it acts as a sum of terms each creating or annihilating minimal pairs of excitations (like an operator that flips a single qubit, which would create or annihilate a pair of syndrome flips on adjacent stabilizers). Such terms of $K$ would allow the system to explore the syndrome graph and ideally find paths to annihilate all excitations. In summary, the full Hamiltonian might look like:
\begin{equation}
    H_{\text{full}} = H_{\mathrm{inv}} + \epsilon K, 
\end{equation}
with $H_{\mathrm{inv}}$ as in Eq.~\eqref{eq:Ham} and $\epsilon K$ a small perturbation facilitating dynamics (with $\epsilon \ll \Delta_i$ to maintain a clear energy hierarchy).

This approach is reminiscent of proposals for \textit{self-correcting quantum memories} \cite{Terhal2015}, where a many-body Hamiltonian naturally protects quantum information. In 2D, no local Hamiltonian can give a truly self-correcting memory at finite temperature (the topological order is destroyed by thermal fluctuations \cite{Alicki2009}), but in higher dimensions or with long-range interactions this may be possible. The notion of inverse topological decoding, however, does not strictly require stability at arbitrarily long times without any intervention; it rather aims to offload the \emph{decoding computation} to the physical substrate. Even if residual errors accumulate slowly, the hope is that hardware-embedded decoding significantly reduces the error rate or extends the memory lifetime compared to a non-protected system, perhaps complemented by occasional high-level error correction cycles.

\subsection{Category Theory and Homotopy Type Theory Perspectives}
Category theory provides an abstract language to discuss structures and processes in quantum computing. The stabilizer formalism itself can be framed in categorical terms (for instance, as a quotient of a free group category by stabilizer relations). More relevant to topological codes is the connection to \textit{topological quantum field theory (TQFT)} and anyon models, which are often described using category theory (e.g., modular tensor categories for anyons \cite{Nayak2008}).

In the context of inverse topological decoding, we are interested in how the logical information and error processes can be understood as morphisms or objects in higher categories:
\begin{itemize}
    \item \textbf{Objects as States:} One might treat the logical sectors (code states) as objects in a category, and physical operations or processes as morphisms. However, since errors and corrections do not change the logical sector (ideally, they act trivially on the logical subspace if corrected properly), one can consider more structure: the category of syndrome configurations.
    \item \textbf{Category of Syndromes:} Consider a category where each object is a possible error syndrome (i.e., a set of stabilizers that have $-1$ eigenvalue). A morphism from syndrome $A$ to syndrome $B$ could represent the physical process of moving from one syndrome configuration to another by applying a certain set of local operations (like moving an anyon). In this picture, a full error-and-correction process is a path (a sequence of morphisms) that starts at the trivial syndrome (no error) and goes out and returns to trivial syndrome (error created and then corrected). The space of all such paths can be considered up to continuous deformations (i.e., homotopies).
    \item \textbf{Homotopy and error equivalence:} Two error paths (sequences of local error events) that lead to the same final syndrome outcome and differ only by a local deformation (like an error loop that can be contracted) are homotopically equivalent in this category of syndrome processes. Homotopy type theory (HoTT) provides a way to treat paths as fundamental objects and equate paths that are continuously deformable. In HoTT terms, one might say we have a type (or space) of all error-correction histories, and the equality relation in that type is given by homotopy of those histories. Fault-tolerance can then be phrased as a statement that all error paths below a certain length (weight) are homotopic to the trivial path (no logical change) so long as they belong to the same homotopy class relative to the boundary (i.e., they are trivial in homology).
    \item \textbf{Higher categories:} The process of error creation, movement, and annihilation can be thought of in terms of a 2-category or higher. For instance, in a 2-category framework, one could have:
        - Objects: the logical information (e.g., different logical states or different code spaces).
        - 1-morphisms: processes or channels acting on these (like an error event which could potentially change the state).
        - 2-morphisms: homotopies between processes (ways one error process can be continuously transformed into another without affecting the end result on logical information).
        
        In such a picture, a logical error (failure of correction) would correspond to a 1-morphism that cannot be deformed to the identity morphism on the logical state --- in other words, a process that represents a non-trivial element of the fundamental group (in a homotopy sense) of the space of error paths. Topologically, these correspond to loops that wind around a hole (for example, an error string connecting two boundaries is not deformable to a point because it is anchored at the boundaries).
        
        Higher-category structures have been discussed in the context of extended TQFT \cite{Baez2011}. In topological codes, one can identify a connection: the syndrome configurations and their evolution can be mapped to a topological picture in one higher dimension (space-time). A classic view is that an error chain in a 2D code over time sweeps out a world-sheet in 3D (2 spatial + 1 time). If that sheet forms a non-contractible surface in the 3D space-time (i.e., it wraps around a handle or connects distant boundaries), it results in a logical error. This can be formalized via homotopy classes of surfaces in space-time. Category theory (in particular 2-categories) can formalize surfaces (2-morphisms) connecting world-lines (1-morphisms).
\end{itemize}

While the above picture is abstract, it provides a rigorous way to reason about fault-tolerance. For inverse topological decoding, we want to ensure that the hardware dynamics only realizes those morphisms (error processes) that are contractible (homotopically trivial) in the space-time picture, or equivalently, any error process that occurs can be ``filled in'' by a correction process to yield an overall trivial effect on the logical information. The Derived Hamiltonian can be thought of as generating allowed 1-morphisms (small local error/correction steps), and the requirement of fault-tolerance is that any such morphisms concatenated together yields either:
\[ \text{(error process)} \circ \text{(correction process)} \cong \mathbf{1}_{\text{logical}}, \] 
the identity on the logical space, if the process is completed. 

Homotopy type theory's notion of paths-as-evidence-of-equality is intriguingly aligned with the idea that if an error is corrected by the hardware, then in a formal proof-theoretic sense that error was ``equal'' to doing nothing (it had no logical effect). We could imagine encoding our entire fault-tolerance proof in a HoTT framework where types represent propositions like "the logical qubit is intact" and certain paths correspond to error-correction operations proving that proposition remains true at later times.

To summarize, category theory and homotopy provide a language to assert that inverse topological decoding works on a high level: any physical error trajectory is either caught and nullified (homotopic to identity on logical info) or, if it's a homotopically non-trivial trajectory, it represents a failure mode beyond the capability of the code. The goal is to push failures to sufficiently low probability by design (making those trajectories high order and energetically suppressed).

\subsection{Higher-Category Structures for Fault-Tolerant Logical States}
We briefly elaborate on the use of higher categories in encoding logical states. Beyond the homotopy-of-error-processes view, higher categories can classify the rich structure of excitations and logical operators in topological systems:
\begin{itemize}
    \item In a topologically ordered phase (like the toric code model), we can speak of a braiding and fusion category of anyons. For example, the surface/toric code has abelian anyons (the $e$ and $m$ $\mathbb{Z}_2$ charges/fluxes) with fusion rules $e \times e = 1$, etc. These anyons and their braiding can be described by a unitary braided tensor category (which is a kind of monoidal category capturing particle exchange statistics) \cite{Kitaev2006}.
    \item If we consider also defects or boundaries, we enter the realm of richer category theory: for example, domain walls between different topological codes can be described by functors between categories, and defects (like holes or twists in color codes) by higher morphisms in an extended category theory picture.
    \item Fault-tolerant logical states, especially in systems with transversal gates or twist-based operations, have been described using concepts from higher categories. For instance, a logical qubit encoded in a non-Abelian anyon pair can be seen as an object in a fusion category, and a braiding operation is a morphism that implements a gate. Extending to 2-categories, one can classify topological quantum computation schemes in terms of functors from the fundamental groupoid of the space (punctured surfaces etc.) to the category of vector spaces (or Hilbert spaces) representing logical states. This is closely related to the notion of \emph{extended TQFTs} assigning states to spatial slices and linear maps to spacetime cobordisms \cite{Walker2012}.
    \item In the setting of error correction, one might imagine a 2-category where:
        - Objects: physical configurations (possibly including locations of anyons).
        - 1-morphisms: operations or evolutions taking one configuration to another.
        - 2-morphisms: equivalences between different sequences of operations.
        
        The logical subspace could then be associated with the set of objects that differ only by vacuum anyon pairs (i.e., those configurations that are actually the same logical state when ignoring localized excitations). Ensuring fault tolerance means that any 1-morphism sequence corresponding to a small error is actually a 2-isomorphic to a trivial 1-morphism on the logical object.
\end{itemize}

These ideas are quite theoretical, but they inform the design: using higher algebraic structures, one could potentially design Hamiltonians or control sequences that explicitly target those structures. For example, certain topological quantum error-correcting codes have interpretations in terms of homological algebra (chain complexes), which is related to category theory. A \emph{homological code} can be specified by a chain complex of vector spaces $C_2 \xrightarrow{\partial_2} C_1 \xrightarrow{\partial_1} C_0$, where cycles and boundaries of this complex relate to stabilizers and logical operators \cite{Bombin2007}. Category theory then tells us that different complexes can yield equivalent logical structures (via quasi-isomorphisms, which could be seen as functors in derived categories).

In summary, while the practical implementation of inverse topological decoding will rely on the Hamiltonian and physical principles, the above mathematical formalism provides confidence that the scheme can be made rigorous. The operator algebra viewpoint ensures a clear definition of the code space and excitations; the derived Hamiltonian defines how those excitations are handled physically; and category/homotopy theory assures us that the global logical information remains coherent by essentially elevating the correction process to a topologically trivial operation in a higher-dimensional space. These tools also guide us in generalizing the approach to more complex codes or even to analyzing the limits of what inverse decoding can do (for example, proving no-go theorems or identifying necessary conditions using topological arguments).

\section{Physics and Hardware-Level Implementations}
Translating the above formal concepts into a working quantum device is a significant challenge. In this section, we discuss how inverse topological decoding might be realized in actual hardware, emphasizing the \textit{real physics} involved. We cover several leading quantum hardware platforms and analyze how the concept of a Derived Hamiltonian and autonomous correction could be embodied in each:
\begin{itemize}
    \item \textbf{Superconducting qubits:} including both flux qubits and transmon qubits, with the use of tunable couplers or multi-body interactions to realize code Hamiltonians.
    \item \textbf{Topological qubits with Majorana fermions:} leveraging the intrinsically protected degrees of freedom in topological superconductors and how inverse decoding plays out in that context.
    \item \textbf{Trapped ions and spin qubits:} exploring whether long-range Coulomb interactions or spin-exchange networks can simulate the needed interactions for inverse decoding.
\end{itemize}
We also discuss experimental feasibility: what has been achieved so far and what near-term experiments could demonstrate aspects of inverse topological decoding.

\subsection{Superconducting Qubits and Circuit QED Implementations}
Superconducting quantum circuits are one of the most advanced platforms for QEC demonstrations to date, including the surface code \cite{Acharya2023}. Typically, these use transmon qubits (weakly non-linear LC oscillators) arranged on a 2D lattice, with nearest-neighbor coupling and ancilla qubits for syndrome extraction. To implement inverse topological decoding, we require instead that the physical couplings themselves enforce the code constraints.

One promising direction is to use \textbf{flux qubits} or other superconducting elements that can realize $4$-body (or multi-body) interactions natively. A flux qubit is a superconducting loop with multiple Josephson junctions where the two-level quantum states correspond to clockwise vs counterclockwise circulating currents. Coupling four flux qubits with a shared junction or mutual inductance could produce a term in the Hamiltonian proportional to the product of the four qubit flux operators \cite{Gladchenko2009}. In principle, this could directly implement a surface code plaquette stabilizer ($Z\otimes Z\otimes Z \otimes Z$ for four qubits around a plaquette) as an energy term.

For transmon qubits, native multi-qubit interactions are not readily available, but one can generate them perturbatively using ancillary coupler circuits. A \textbf{tunable coupler} (often another Josephson junction circuit that mediates interaction between two qubits) can be driven or biased such that effective 2-qubit Ising terms $Z_i Z_j$ are realized. By combining multiple 2-body couplers and possibly higher levels of perturbation or driving, an effective 4-body term can be engineered. For example, one scheme might connect a central coupler to four data qubits; in second-order perturbation theory, the combination of those interactions yields an energy shift proportional to the product of the four qubit $\sigma^z$ operators (since the coupler can virtually transition if an odd number of attached qubits are in $|1\rangle$, thus disfavoring certain parity configurations) \cite{Xu2022Coupler}. This is analogous to techniques used in quantum annealing hardware to embed higher-order constraints into 2-body Ising interactions by introducing auxilliary variables (the so-called gadget approach in Hamiltonian complexity).

Another approach in superconducting systems is to realize the \textbf{stabilizer Hamiltonian through driven interactions or resonator modes}. In circuit QED, qubits coupled to a cavity can experience multi-qubit interactions via exchange of virtual photons. If four qubits couple to a common resonator mode, a driving of that mode can induce an effective four-body Hamiltonian in a rotating frame (this can be derived via fourth-order perturbation or Floquet theory). There have been proposals to simulate lattice gauge Hamiltonians (like the toric code model) using arrays of superconducting circuits with microwave driving \cite{Kapit2016}. These often involve a combination of capacitive and inductive couplings to get both $X$-type and $Z$-type stabilizer terms.

In terms of real physics, implementing $H_{\mathrm{inv}} = -\sum S_i$ means the system's lowest energy manifold is the code space. For a small distance code, say a $d=3$ surface code (which involves $n = 9$ data qubits and some number of plaquette/star operators), we might attempt a proof-of-principle: nine qubits on a 3x3 grid with carefully tuned couplings such that the Hamiltonian has degenerate ground states corresponding to the two logical states of the code. One would then introduce an excitation (by flipping a qubit) and see if the system naturally returns to a ground state (perhaps via coupling to a dissipative bath mode engineered for syndrome removal as mentioned earlier). There has not yet been an experimental demonstration of a Hamiltonian protecting a multi-qubit logical qubit in superconducting circuits, but steps in this direction include smaller-scale experiments: for instance, two-qubit parity protection (where a $ZZ$ interaction is used to preserve parity against single qubit flips) or cat-qubits in cavities where two-photon drives create an effective stabilizer $a^2$ (photon-pair creation) that autonomously corrects single-photon loss errors \cite{Lescanne2020}. These cavity cat codes are a form of hardware error correction using a harmonic oscillator, showing that with proper engineering, a system can resist certain errors autonomously. In our case, the challenge is greater: we need to protect a \emph{distributed, entangled code} rather than a single-mode cat state.

The use of \textbf{dissipation engineering} in superconducting circuits could involve adding filters or resistive elements that specifically damp modes corresponding to syndrome excitations. One idea is to have each stabilizer associated with an ancilla resonator that detects its violation and then dumps that energy into a phonon or photon that leaves the system. This would effectively cool the syndrome excitation. Superconducting devices often try to be as isolated as possible to preserve coherence, so deliberately coupling to a bath must be done in a targeted way (to avoid introducing general decoherence). One could use something like a low-Q resonator tuned to the frequency of a stabilizer violation, which then radiates out of the fridge a photon carrying away the error syndrome.

In summary, for superconducting qubits:
\begin{itemize}
    \item The hardware can in principle implement derived Hamiltonians using networks of qubits and couplers. Flux qubits provide one possible direct route with multi-body inductive couplings.
    \item Transmons require clever multi-step coupling schemes; experimental complexity grows with the number of qubits in a stabilizer.
    \item Some simpler forms of autonomous error correction (e.g., qubit parity or cat code stabilization) have been achieved, giving confidence that incremental progress toward a full topological code Hamiltonian is possible.
    \item A near-term demonstration might involve a small plaquette (say 4 qubits with one stabilizer term) being stabilized, or a $d=3$ surface code memory being protected for longer than the best single qubit. 
\end{itemize}

\subsection{Majorana Fermions and Topological Qubits}
Majorana-based qubits are fundamentally rooted in topological protection. In a topological superconductor (such as a semiconductor nanowire with strong spin-orbit coupling in proximity to an s-wave superconductor, under appropriate magnetic field and chemical potential conditions), zero-energy Majorana bound states can emerge at the wire ends \cite{Alicea2012}. Each pair of Majoranas encodes one qubit non-locally (the parity of occupied states in the two Majorana modes defines a two-dimensional Hilbert space). The attractive feature is that any local disturbance (one that cannot simultaneously affect both ends) cannot flip the qubit, because flipping the qubit requires changing the parity which is a non-local operation spanning the two Majoranas.

In such a system, some aspects of inverse topological decoding are \emph{intrinsic}: the hardware already has a built-in topological protection mechanism. Braiding of Majoranas (exchanging their positions adiabatically) yields unitary operations on the encoded qubit that are topologically robust to small errors. However, Majorana qubits are not a complete error correction solution on their own. They primarily protect against certain types of errors (those that preserve fermion parity in each wire). They are still susceptible to, for example, quasi-particle poisoning (an unwanted electron enters the system and changes parity), or simply to the fact that braiding alone is not universal for quantum computation and needs to be supplemented by other operations.

How could we leverage Majoranas in an inverse decoding context? One possibility is to build a \textbf{Majorana-based code}: arrange many Majorana modes into a network that can support a larger logical code. For example, proposals exist for a Majorana surface code \cite{Terhal2012MajoranaSurf}, where Majorana modes on a 2D grid realize effective stabilizers (parity constraints). The advantage would be that some of the interactions are naturally present (the parity of four Majoranas might be a stable quantity if they are connected by superconducting islands). One could imagine a hybrid approach: the code's stabilizers are parity checks of Majorana modes, and a superconducting circuit or classical circuit measures/corrects those. But for inverse decoding, we would want the parity check violations (which correspond to anyons or excitations in a Majorana surface code) to be automatically corrected.

At the hardware level, one can consider a device made of multiple topological superconducting islands. For instance, each plaquette in a code could be a superconducting island hosting four Majoranas at its corners (via wire junctions). The total parity of those four Majoranas (two electrons modulo 2) could act as a $\mathbb{Z}_2$ stabilizer. If a Majorana pair on a link between plaquettes changes parity (i.e., an anyon is created), it might propagate along the wire. Designing a \textbf{network of nanowires} with tunable couplings (gates controlling tunneling between Majorana sites) could allow moving the anyonic excitations. By tuning these couplings dynamically or even just through equilibrium, the system might favor having no excitations.

A concrete example: a Majorana-based qubit is often physically realized as a superconducting Coulomb island with two Majoranas. If one connects two such islands in a certain way, one gets four Majoranas whose combined parity is fixed by a charging energy term (Coulomb energy imposes an even total parity on each island). This yields an effective interaction among the Majoranas (in fact, a 4-Majorana term in the Hamiltonian). This is precisely a stabilizer: the parity operator (product of two fermionic creation operators, which can be mapped to a product of Pauli operators in a certain encoding) is fixed. Scaling this up, networks of Majoranas can emulate the toric code Hamiltonian in a fermionic version \cite{Vijay2015}. 

The real physics challenge for Majoranas is that while they give passive protection against some local errors, they are still not error-proof in the long run. Inverse decoding would mean designing the device so that if, say, an unwanted quasiparticle appears (parity changes), there is a mechanism to remove it or to shuffle it out (like a quasiparticle trap absorbing stray fermions). Researchers have considered including normal-metal traps in Majorana devices to soak up poisoning events \cite{Karzig2017}. This is somewhat analogous to the engineered dissipation idea: you create a preferential sink for errors.

Another aspect is that Majorana devices currently are at a relatively early stage of development: braiding has yet to be conclusively demonstrated in experiment, and coherence times are not yet at the level of transmon qubits. So in the near term, an \emph{all-Majorana topological quantum computer} with inverse decoding remains speculative. However, hybrid approaches might come sooner, for instance:
\begin{itemize}
    \item Use Majorana qubits as qubit memories (they are small and possibly can be packed densely).
    \item Use superconducting qubits or other means to do operations or to facilitate corrections that Majorana alone can't do.
    \item The combination might reduce overhead if each Majorana qubit is already partially protected (so maybe one can build a smaller code from such qubits to correct the remaining errors).
\end{itemize}

In summary, Majorana fermion platforms naturally embody some of the principles of inverse topological decoding (intrinsic error suppression), but to incorporate them into a full error correction scheme at hardware level, significant engineering (networks of many Majoranas, quasiparticle management, and measurement or braiding control) is needed. The physics here involves superconductivity, materials science (semiconductors, nanowires, 1D heterostructures), and even electrostatic effects (charging energy for parity enforcement). This is a very different regime from superconducting transmon circuits, illustrating the broad scope of possible hardware.

\subsection{Trapped Ions and Spin Qubits}
Trapped ion quantum computers have achieved high-fidelity gates and are attractive for QEC because of their long coherence times and all-to-all gate connectivity (via collective motional modes). To date, trapped ions have shown implementations of small quantum error-correcting codes (Shor code, Steane code, repetition code) using sequences of gates and measurements \cite{RyanAnderson2021}. Could we use trapped ions to simulate a derived Hamiltonian for a topological code?

One idea is to use the fact that ions can interact via effective spin-spin interactions mediated by phonons. By tailoring laser drives, one can implement an Ising Hamiltonian $H_{\text{Ising}} = \sum_{i<j} J_{ij} X_i X_j$ (or $Z_i Z_j$ depending on basis) across the ion chain with tunable coefficients $J_{ij}$ \cite{Monroe2019}. If we have a 2D code, mapping it to a 1D chain is non-trivial, but ion trap architectures are exploring 2D arrangements of ions in trap arrays or shuttling ions between traps to effectively create 2D connectivity. Alternatively, one might envision encoding the topological code in a single chain's interaction graph (not physically 2D, but in graph theory terms using all-to-all connections one could embed a surface code's stabilizer graph onto an ion chain with long-range couplings).

To implement, for example, the toric code Hamiltonian, we need four-body $Z$ stabilizer terms and four-body $X$ stabilizer terms. With ion trap, you might simulate this by a series of two-body interactions and possibly Trotter decomposition. However, truly continuous Hamiltonian protection might be attained by \textbf{Floquet engineering}: cycle through different pairwise couplings rapidly so that the time-averaged effect is a multi-body stabilizer. For example, an echo sequence could effectively realize $Z_1Z_2Z_3Z_4$ by successively coupling pairs in a certain pattern and doing rotations.

Another interesting possibility with ions is using multi-ion entangled states as resources: one can directly create a four-ion GHZ state (which is a stabilized by $XXXX$ and $ZZZZ$ on those four ions) by a sequence of gates. If one had a way to continuously drive the system towards that GHZ state, that is like an autonomous $XXXX$ stabilizer. In fact, researchers have demonstrated stabilization of entangled states in ions using measurement and feedback \cite{Negnevitsky2018}, but not yet purely Hamiltonian-based stabilization. But because measurement and feedback can be fast in ions (with certain setups), one variant of inverse decoding could be a very rapid active feedback within the hardware's control system that is effectively integrated (blurring the line with truly autonomous but could be considered hardware-level since it might be an analog or FPGA control loop locally in the trap).

Spin qubits in solid state (like spins in silicon quantum dots or NV centers in diamond) present other considerations. Solid-state spins can have long coherence (NV centers) or be manufactured in large numbers (quantum dots in silicon or gallium arsenide). For quantum dots in silicon, two-qubit gates are typically nearest-neighbor exchange interactions. Realizing a multi-qubit code would require a 2D network of dots with couplers or long-range interactions (maybe via cavity photons or shuttling electrons). It's quite challenging with current technology to imagine multi-body Hamiltonians directly in quantum dot spins. However, one could conceive a design where a quantum dot array is coupled via a microwave cavity, generating some effective long-range coupling that might be harnessed for stabilizer enforcement (similar to circuit QED but with spins). Material constraints (e.g., temperature, since many spin qubits operate at millikelvin as well) and fabrication (small feature sizes to couple many dots) are major factors.

Trapped ions are closer to being able to simulate arbitrary interactions thanks to their reconfigurability via laser control. Actually, there has been work on mapping Z2 lattice gauge models (which essentially is the same math as a topological code) onto ions or Rydberg atoms \cite{Homeier2023Ry}. Those experiments or proposals target seeing anyon dynamics or phases, which is directly relevant. If such a simulation can run continuously, it is effectively a hardware error-corrected memory: the ions mimic the code's Hamiltonian and if one ion flips, it costs energy so ideally it flips back.

\subsection{Experimental Feasibility and State of Progress}
As of the time of writing, fully realized inverse topological decoding has not been achieved in any platform. However, several key milestones suggest it is a plausible goal:
\begin{itemize}
    \item \textbf{Small stabilizer Hamiltonians:} In superconducting circuits, 2-qubit and 3-qubit parity protection experiments have been done. For example, two transmons coupled by a fixed $ZZ$ interaction show slower decoherence in the subspace of fixed parity, because single-qubit flips (which change parity) are suppressed or detectable. Similarly, a 3-qubit repetition code has been maintained passively for a short time using engineered dissipation in a superconducting setup \cite{Leghtas2013}.
    \item \textbf{Bosonic codes with autonomous correction:} The cat code using a superconducting cavity resonator has demonstrated that a logical qubit encoded in two coherent states can be stabilized against single photon loss by a two-photon drive, effectively an always-on correction mechanism \cite{Ofek2016}. This is not a topological code (it's a single oscillator), but it proves the principle of an analog Hamiltonian (or engineered interaction) protecting quantum information.
    \item \textbf{Anyons in the lab:} Interference experiments in the fractional quantum Hall effect have shown signatures of anyonic statistics, and recently, nanofabricated devices aim to braid anyons in those systems \cite{Nayak2008}. While not used for qubits yet, they are physical realizations of topological phases. If one can control them, that is like having a naturally error-corrected space (the anyon world).
    \item \textbf{Majorana modes:} Zero-bias peaks consistent with Majorana bound states have been observed in multiple superconducting-semiconductor devices \cite{Mourik2012}. Although not yet fully conclusive, if these are indeed Majoranas, they are the building blocks of topologically protected qubits. Scaling up to networks is an active research area, e.g., Microsoft Quantum has pursued a design where many Majorana pairs are interconnected on a superconducting quantum dot array \cite{Karzig2017}.
    \item \textbf{Trapped ion QEC:} Though current trapped ion experiments use measurement-based QEC, they have laid groundwork for controlling 10-20 qubits with high fidelity, and even real-time feedback. Demonstrations such as a 5-qubit code correcting an error or a 7-qubit code preserving a state for longer than any single qubit (the ``break-even'' point) have been reported \cite{RyanAnderson2021}. These are active processes, but suggest that with some modifications one could attempt a Hamiltonian approach.
    \item \textbf{Quantum simulations:} Digital and analog quantum simulations of spin models that are analogous to QEC codes have been done in small scales. For instance, a 4-qubit toric code Hamiltonian has been simulated on an NMR quantum simulator to study its energy spectrum \cite{Lu2017NMR}. Rydberg atom arrays have been used to simulate $\mathbb{Z}_2$ gauge theories (where the presence or absence of a Rydberg excitation on a link can be thought of like a bit of a stabilizer) \cite{Homeier2023Ry}. These physics experiments are not yet protecting a qubit, but they validate that we can engineer those Hamiltonians in principle.
\end{itemize}

The near-future setups needed to test inverse topological decoding might involve:
\begin{enumerate}
    \item \textbf{A small logical qubit demonstrator:} e.g., 4 or 5 physical qubits encoding one logical qubit with one or two stabilizers (like a baby surface code of distance 2 or 3). The hardware should implement $H_{\mathrm{inv}}$ (or an effective version) and an apparatus to introduce a controlled error and see if it heals. The success metric would be an extended lifetime of the logical qubit as compared to no protection. This could be done in superconducting or ion platforms within a few years, given progress in multi-qubit interactions.
    \item \textbf{Dissipative error removal:} an experiment deliberately injecting an excitation (e.g., flipping a stabilizer) and demonstrating an element of the system (like a lossy resonator or a specific protocol) removes that excitation without knowing which one was flipped. This would simulate the decode action physically.
    \item \textbf{Hybrid setups:} Integration of a small Majorana qubit with a transmon for reading out. If one can show that a Majorana qubit has longer T1 or T2 (lifetime, coherence) than a transmon under similar conditions, that already hints at the power of physical topological protection.
    \item \textbf{Materials and coherence improvements:} Achieving a regime where the engineered interactions ($\Delta$ in Eq.~\ref{eq:Ham}) are strong compared to noise rates. If $\Delta$ is too small, thermal excitations will constantly appear. So one needs strong coupling energy and low temperature. For superconductors, that means improving coherence so that large $J$ couplers don't introduce more noise. For Majoranas, it means a substantial mini-gap for quasi-particles.
\end{enumerate}

All these efforts require cross-disciplinary cooperation: quantum information theorists to design the code and decoding concept, experimental physicists to implement and refine it, and engineers to solve practical issues of scaling and integration. We now turn to some of those engineering aspects.

\section{Engineering and Fabrication Considerations}
Implementing inverse topological decoding principles in a quantum processor imposes demanding requirements on engineering and fabrication. Here we discuss feasibility and challenges:
\begin{itemize}
    \item \textbf{Fabricating multi-qubit interaction networks:} We need to build systems where multiple qubits interact in a controlled fashion, beyond simple pairwise connections.
    \item \textbf{Cryogenics and thermal considerations:} Many of these systems operate at millikelvin temperatures. If error suppression relies on an energy gap, maintaining a temperature well below that gap is crucial.
    \item \textbf{Material constraints:} The choice of substrate, superconductors, or other materials can impact coherence and ability to create needed couplers.
    \item \textbf{Scalability and integration:} We consider how to scale such systems up to many logical qubits.
\end{itemize}

\subsection{Feasibility of Fabricating Quantum Processors with Inverse Decoding}
From a fabrication standpoint, a device implementing a derived Hamiltonian might look different than a standard gate-based quantum chip. In a gate-based chip (like current superconducting or spin qubit chips), one typically has an array of qubits and a network of wiring to send control pulses to each qubit and readout resonators to measure them. For a Hamiltonian-based approach, one might not need as many fast control wires for single-qubit rotations or measurements (if the idea is to mostly rely on passive physics), but one would need a robust fabric of inter-qubit couplers embedding the code. 

\textbf{Superconducting example:} A surface code lattice Hamiltonian in hardware might involve a planar array of superconducting loops where each loop or cell implements a stabilizer (like a plaquette operator). Josephson junctions or mutual inductances between loops would be patterned such that each qubit (or loop) participates in the four adjacent stabilizers. This is similar to how one would lay out the syndrome measurement circuit, but here the connections have to be analog, not just measurement lines. This implies a high degree of uniformity and precision in coupling. The feasibility of lithographically fabricating such an array is plausible given current technology (we can make 2D arrays of thousands of Josephson junctions, as in superconducting memory or detector arrays). The main challenge is ensuring low disorder (variations in junction critical current could make some stabilizer stronger than others, etc., which could be problematic if not calibrated). Another difficulty is packaging: the chip might need to integrate resistive elements or resonators for dissipative error removal, requiring heterogeneous integration of different circuit elements.

In terms of yield and complexity, adding couplers and multi-body interactions means more components on chip. For example, if an ancilla coupler is needed for each stabilizer (like a 4-qubit stabilizer might use one coupler qubit), that is overhead in physical qubits beyond just the data qubits. It becomes reminiscent of a measurement-based scheme with ancillas, except these ancillas operate in analog mode. So hardware overhead might actually increase (since in measurement-based, ancillas can be reused sequentially, whereas in a static Hamiltonian they might need to be present all the time).

\textbf{3D integration:} One solution to complexity is 3D integration, where multiple layers of wiring or even multiple layers of qubits are stacked. IBM and others have begun using through-silicon vias and multi-layer wiring to route signals to large arrays without crossing wires on the same plane. For an inverse decoding chip, one might similarly route, say, a control or bias line that tunes all stabilizers of a certain type through layers. Perhaps one layer carries the $Z$ stabilizer couplers and another the $X$ stabilizer couplers to avoid interference.

\subsection{Cryogenics and Thermal Considerations}
All topological code implementations suffer if the temperature is too high relative to the stabilizer energy penalty. For a derived Hamiltonian, this means $k_B T \ll \Delta$ is needed to keep the equilibrium state in the code space. In practice, dilution refrigerators can reach $T \approx 10$ mK. What is a realistic $\Delta/h$ (in frequency) one can have? Possibly in superconducting circuits, $\Delta$ could be on the order of a few GHz at most (since qubit energy splittings are ~5 GHz, and coupling strengths are typically at most ~100 MHz in current devices, though with stronger coupling qubits or resonators maybe approaching GHz is possible). $k_B T$ at 10 mK is about $200$ MHz$\times h$ (since $1$ K is $20.8$ GHz, so $0.01$ K is $208$ MHz). So if $\Delta$ is 1 GHz, that's comfortably above $k_B T$. If $\Delta$ is 50 MHz, then thermal excitations would be significant. Therefore, cryogenic improvement or ensuring strong coupling is crucial.

One must also consider heat dissipation: if our scheme uses engineered dissipation to remove errors, that means the system is intentionally generating excitations (photons, phonons) that carry away entropy. In a cryostat, if many qubits are doing this, can the cooling power handle it? This becomes an engineering question: each error removal might release a photon of frequency $\Delta$ to some dump resistor, which then must be cooled. If the error rate is low, it's fine; if we scale to thousands of qubits, and each might produce e.g. 10 excitations per second, that could be thousands of photons per second at maybe 5 GHz frequency, which is a tiny power (picoWatts) easily absorbed. However, if something goes wrong and a lot of excitations happen (like at threshold of a breakdown), it could produce bursts of heat.

Material constraints for cryogenics: often one uses copper or gold as good thermal anchors. If we incorporate normal metal (for dissipation), that adds a thermal link that could bring unwanted heat if not properly isolated by filters.

\subsection{Materials and Fabrication Challenges}
Materials affect coherence. For instance, superconducting qubit coherence is limited by two-level system (TLS) defects in amorphous oxide tunnel barriers of junctions, and by surface losses. If we dramatically increase the number of junctions (like building a big coupled array), we may introduce more loss unless design changes (e.g., use larger junctions with smaller participation of lossy oxide). So one solution could be designing junctions differently or using capacitive coupling rather than direct junction coupling to minimize new loss channels.

For Majorana devices, materials are a huge challenge: combining semiconductors (InAs, InSb nanowires or epitaxial heterostructures) with superconductors (Al) and making complex networks with gates and readouts. The yield and uniformity of these is still low. Even if a single wire works, scaling to dozens might have failures. There is progress in 2D materials (like epitaxial superconductor on semiconductor 2D sheets that can be patterned into networks).

Integration of Majorana devices with conventional circuits is another issue: they need magnetic fields to induce the topological phase, whereas superconducting circuits generally try to avoid fields. But one can use materials that are less field-sensitive (NbTiN, for example, can sustain higher fields than Al). Alternatively, use vector magnets that apply fields locally only to the nanowire region.

Trapped ion hardware has very different fabrication considerations: building advanced traps (with many zones or 2D traps), integrating on-chip optics or current-carrying wires for dynamic coupling. It's a whole field of its own. But they can operate at room temperature or moderate vacuum environment, which is simpler in some ways. The integration challenge there is controlling many laser beams or having integrated photonics for addressing many ions in parallel, if we wanted to create complicated interactions simultaneously.

\subsection{Proposed Solutions and Innovations}
To overcome the challenges, researchers are exploring:
\begin{itemize}
    \item \textbf{Novel coupler designs:} like the fluxonium qubit (a superconducting qubit with large inductance) which can mediate stronger interactions or be more noise-free at lower frequencies. 
    \item \textbf{Topological materials for superconducting circuits:} using materials that intrinsically have fewer defects, such as crystalline Josephson junction barriers, or eliminating dielectric layers.
    \item \textbf{Precision calibration and tunability:} designing each coupler to be tunable so that post-fabrication, one can dial in the correct coupling strengths to realize the code Hamiltonian precisely (compensating for fabrication spread). This is akin to how D-Wave (quantum annealing company) calibrates thousands of analog couplers in their Ising machines to mitigate fabrication variations. Inverse decoding hardware might borrow techniques from quantum annealers, since those are essentially large analog Hamiltonians too.
    \item \textbf{Thermal cycle and vacuum improvements:} Ensuring that the system can remain ultra-cold with minimal vibrations and that any introduced dissipative elements do not warm up the chip excessively. Possibly using cold finger setups, dedicated cooling for certain parts of the chip, or in cryo electronics to handle heat separate from the qubit chip.
    \item \textbf{Error-tolerant design:} Ironically, one might apply error correction ideas to the hardware design itself: e.g., if one coupler fails, have redundancy; if one stabilizer is slightly off, the others still preserve the code (error correction can tolerate some imperfection).
    \item \textbf{3D topological codes:} Considering building a 3D structure (like a stack of chips) that physically implements a 3D code might yield better self-correction. For example, a 3D color code or a 4D toric code cannot be directly fabricated in 4 spatial dimensions, but one could simulate a 4D code by using time or using multiple 3D arrays with interactions between layers to mimic a 4D lattice. These exotic schemes might be far-fetched now, but if needed, could be a direction to ultimately achieve a stable quantum memory at finite temperature.
\end{itemize}

Overall, from an engineering perspective, the key question is whether the increased hardware complexity of an inverse-decoding approach is justified by the reduction in complexity of control and classical processing. It shifts the burden: rather than having fast electronics to decode and correct every few microseconds, we put that burden on fabrication and analog design to do it continuously. In practice, the optimal solution might be a hybrid: some level of autonomous protection combined with occasional measurements to catch rare events that the analog system can't correct (for instance, a major uncorrectable error could be caught by a periodic check).

In the next section, we analyze some of these trade-offs from a computational perspective, comparing complexity and overhead.

\section{Computational and Theoretical Implications}
Beyond the immediate physics and engineering, inverse topological decoding invites analysis in terms of computational complexity theory and quantum fault-tolerance theory. Here we consider how this approach compares to conventional QEC methods in terms of:
\begin{itemize}
    \item \textbf{Decoder complexity vs. Hamiltonian complexity:} Classical decoders for topological codes (like minimum-weight perfect matching for surface code) have polynomial complexity in the number of qubits. What is the "complexity" of solving the same problem via physical dynamics? Are we trading a classical computational problem for a physical equilibration problem?
    \item \textbf{Error thresholds and fault-tolerance:} How might the threshold error rate or overhead (number of physical qubits per logical qubit) change under this approach? Does the presence of an always-on correction mechanism improve effective error rates?
    \item \textbf{Scalability:} Fault-tolerance in the standard circuit model has a clear threshold theorem. Does an analog error-correcting system obey a similar threshold principle? What are the new potential failure modes?
\end{itemize}

\subsection{Complexity Analysis: Physical vs. Classical Decoding}
In a conventional surface code, when a quantum error occurs and syndromes are measured, the task of the classical decoder is to interpret the pattern of syndromes (usually a collection of anyons) and pair them up appropriately to propose a correction. The problem of finding the most likely error chain given a syndrome is equivalent to a minimum-weight perfect matching (MWPM) problem on a graph whose vertices are the anyons and edges weighted by distance or error likelihood. MWPM can be solved in roughly $O(n \log n)$ time for $n$ anyons or even faster with recent improvements (almost linear in number of qubits for low density of anyons) \cite{Higgott2022}. This is efficient, but implementing it in real time in hardware as codes scale to millions of qubits is non-trivial due to communication and processing constraints at cryogenic temperatures.

Inverse topological decoding effectively uses the analog dynamics of the qubit array to perform a similar pairing up of anyons. One can think of it as using physics to compute the matching: anyons feel an attraction (energetic drive to annihilate) and will thus find a partner to annihilate with (not necessarily the absolute optimal partner, but likely the nearest one, which is indeed the minimal weight solution in many cases). In some sense, the system is performing a greedy local algorithm for decoding by letting anyons follow the gradient of energy (which tends to bring them together). If the anyon gas is sparse (errors are rare), then presumably they will annihilate with the nearest neighbor which is correct. However, one must consider the possibility of getting trapped in a suboptimal configuration, analogous to a local minimum in an optimization landscape.

From a computational complexity viewpoint, the Hamiltonian $H_{\mathrm{inv}}$ is a sum of commuting projectors (if we take $S_i$ being $\pm 1$ projectors) which is trivially diagonalizable, but the addition of anyon hopping terms $K$ makes the dynamics nontrivial. It becomes similar to a quantum or thermal annealing process solving a matching problem. For simple noise models like uncorrelated Pauli errors, the optimal decoding is given by matching, but the physical process may approximate that if kinetics allow.

It is interesting to ask: could the decoding problem ever be NP-hard? For general quantum codes or for adversarial error patterns, decoding is NP-hard, but for topological codes with stochastic independent noise, it is efficient. So we are not expecting to solve an NP-hard problem by physics here (which is good, because likely the physical process would fail if it were NP-hard). Instead, we are trying to eliminate the need for the classical solver by a continuous process that is naturally parallel and potentially faster (since it happens at the speed of system's dynamics).

One should be careful though: the physical process has no lookahead or global knowledge. It is possible for the system to settle into a state with multiple anyon pairs separated that could annihilate differently. For example, imagine four anyons in a line: the optimal is to pair the two left ones and two right ones, but if the system randomly pairs a left with a right across the middle, it might form a long string (which could be a logical error if that winds around a hole or boundary). Would the system realize the mistake and rearrange? In principle, thermal fluctuations might allow re-pairing, but if energy is only sensitive to total number of anyons, any pairing is the same energy until they actually annihilate. This indicates a potential need for some specific design that biases the system towards shorter error loops.

One possible mechanism is to have a slight attractive force or potential increasing with distance between anyons: in some models of anyons (like in a $\mathbb{Z}_2$ gauge theory), anyons are deconfined (free) in pure toric code, but if one introduces a small confining potential (like a weak magnetic field term that makes separating charges cost a bit more energy), anyons would feel a tendency to come together. This could help the analog decoder avoid making long corrections. However, one must ensure not to break the topological degeneracy inadvertently. This becomes a fine tuning issue.

In terms of computational steps, a classical decoder might take microseconds to decode a large patch after each syndrome extraction cycle. The analog scheme would be continuously doing it each time an error happens. If error rate is low, anyons annihilate quickly well before the next error appears. If error rate is near threshold, many anyons might be present and the system could struggle (like traffic jam). The threshold in an analog scheme might be determined by a competition between error creation rate and error annihilation rate. If errors are too frequent, anyons proliferate and overlap before they can annihilate, leading to a breakdown (like a phase transition to a disordered phase). In a classical scheme, threshold is determined by whether an infinite chain percolates. In a physical scheme, it's like a non-equilibrium phase transition in a driven-dissipative system.

This would be a fascinating study: mapping the error correction threshold to a physical percolation or phase transition threshold. Possibly similar to the finite temperature phase transition of the toric code in a field (which is known to be at $T=0$, meaning no finite T is stable in 2D; similarly if errors come in continuously (like finite T), the system cannot handle it beyond zero threshold if no external help. But if we allow the designed dynamics (non-equilibrium, not just thermal, since we might be actively driving annihilation), maybe there's an effective threshold. Some prior work in the context of \textit{dynamically generated codes} (like continuous error correction circuits) could relate.

In terms of big-O complexity, the analog approach might not be easily described by polynomial time, but rather by physical time and energy scales. If we double the code distance, classical decoding time maybe grows poly(d), whereas analog decoding speed might slow because anyons have to travel further on average. But since they travel concurrently, it could still be fast (diffusion times might scale as distance squared if random, or ballistic if engineered).

To summarize: complexity-wise, inverse decoding replaces a classical computation with a physical relaxation process. Provided the noise rates and system size are within a certain regime, this should be efficient and maybe faster in practice, but analyzing worst-case or large-size behavior requires thinking in terms of many-body physics and phase transitions rather than standard complexity classes.

\subsection{Fault-Tolerance and Threshold Considerations}
The quantum fault-tolerance threshold theorem states that if the physical error rate per gate or per time step is below a threshold value, arbitrarily long quantum computation is possible with polylog overhead. For surface codes, this threshold is on the order of $1\%$ for error-per-gate in circuit models, or per time step error in memory (though refined analyses show $\sim0.1\%$ depending on details \cite{Fowler2012}). In a hardware-embedded scheme, we should identify an analogous criterion:
- The physical error processes include spontaneous qubit errors (bit-flips, phase-flips) at some rate $\lambda$.
- The correction mechanism has some strength (e.g., an anyon pair tends to annihilate with rate $\Gamma$ once formed, maybe proportional to coupling $\epsilon$ or coupling to bath).
- If $\lambda \ll \Gamma$ (errors are corrected faster than new ones appear), the memory will be stable. If $\lambda \gg \Gamma$, errors accumulate and cause logical faults.

Thus the threshold might be roughly when $\lambda/\Gamma$ crosses some critical value. We might define an effective error per correction cycle ~ $\lambda / \Gamma$ analogous to the ratio per QEC round threshold in standard codes.

Additionally, in circuit-based QEC, we consider errors during the measurement and operation of the code itself. In our analog approach, operations are continuous, so we consider errors in the couplers or environment as well. For instance, if a coupler fails (like it has some chance to create a pair of anyons spontaneously because of its own noise), that's an additional error channel. So building the code into hardware does not remove the need for reliability of that hardware. We might think of the stabilizer operations themselves needing to be fault-tolerant. If a coupler glitch causes flipping of a stabilizer (like it dumps an anyon pair spontaneously), that is similar to syndrome extraction errors.

One could incorporate redundancy to mitigate coupler errors. Or rely that such events are rare relative to qubit errors hopefully.

If the analog system meets threshold conditions, then we expect an exponential suppression of logical error with increasing code distance $d$, similar to usual: $\sim (\lambda/\Gamma)^{(d/2)}$ or something if each step a error chain of length $d$ is unlikely.

However, a potential caveat: in standard codes, increasing $d$ means more qubits, so more physical error locations too, which somewhat offsets the benefit, but net is exponential suppression if below threshold. In analog, if we simply enlarge the code (say bigger area, same physical error rate), more anyons might appear, but the distance they need to go to cause logical is larger. Likely similar outcome: below threshold, the chance of an uncorrected chain crossing the whole code decays exponentially in $d$.

We should consider how gate operations (for computing, not just memory) work under this scheme. Fault-tolerance demands that we can perform gates without losing the protection. If the Hamiltonian is always on, performing a logical gate might involve manipulating the code gently (like moving anyons or changing boundary conditions). We must ensure those manipulations don't break the gap or introduce vulnerability. Possibly, one would temporarily adjust some couplings to allow an anyon pair to be dragged across (performing a braid or something). This must be done slower than the gap to remain adiabatic or using an error path that keeps things under control. This could slow down gate operations relative to a purely active scheme, which is a trade-off: we get easier error correction but maybe slower gates.

Alternatively, one might turn off $H_{\mathrm{inv}}$ during certain operations to do gates faster and turn it back on. But turning it off exposes the system temporarily.

Another theoretical implication: if one had a fully error-correcting Hamiltonian in the passive sense, it might allow a form of quantum computing without measurement overhead, but one might still need measurement for a universal gate set (like magic state injection if using surface code, since transversal gates are limited).

In terms of scalability: a positive implication is that if hardware can decode itself, the communication and latency overheads are reduced. This might simplify the architecture as we scale to 1000s of qubits. One of the concerns for large QEC is the network of classical wires and controllers might be very heavy (imagine millions of signals coming out of a cryostat at GHz speeds to decode surface code syndrome in real-time). If instead the chip mostly handles it internally, the external control could be slower and simpler (just occasionally check if everything is okay or apply a global reset if not).

On the flip side, scaling analog systems is tricky because calibrating and maintaining uniform conditions across a large chip is hard. Also, if each logical qubit is an analog memory, how do we connect them for multi-qubit gates? Possibly using some intermediate connecting hardware (like a bus resonator) that itself might break the local error correction if not careful.

One could envisage a modular design: each logical qubit (say a distance-5 surface code tile) is one module with its own $H_{\mathrm{inv}}$ protecting it. To entangle two logical qubits, maybe bring them adjacent or have an interface where one can braid an anyon from one module to another. That must be studied for error generation.

Finally, there's an interesting theoretical implication about the nature of noise: if we achieve a system where error correction is done by physics, have we essentially created a new phase of matter? Specifically, a phase that is a non-equilibrium steady-state supporting quantum information. If stable, that might be akin to a \textit{dissipation-protected qubit phase}. People have spoken about \textit{passively protected quantum memory as the holy grail} but known no-go results say 2D can't at finite T. But we are adding non-equilibrium (drives, feedback). So we might circumvent equilibrium no-go by having an active stabilizer drive (which basically is adding energy constantly to correct errors, like a refrigerator extraction of entropy).

This overlaps with the concept of a \textit{quantum memory with self-correction} extensively studied in theory (e.g., 4D toric code, 3D cubic code, etc. which are not yet practical but theoretically self-correct at some T).

In conclusion, theoretically, inverse topological decoding stands at the crossroads of quantum error correction, condensed matter physics (topological phases), and non-equilibrium statistical mechanics. It has the potential to simplify certain aspects of fault-tolerant quantum computing, but it introduces new considerations in system dynamics and error sources. It's not a free lunch: some complexity is shifted into analog engineering. But if done right, it could significantly improve the practical viability of large quantum computers by reducing the reliance on ultra-fast, complex classical control systems.

\section{Practical Implementations and Future Outlook}
Having laid out the theory, the hardware prospects, and the implications, we now turn to the practical path forward. How can inverse topological decoding be integrated into modern quantum computing architectures, and what intermediate steps will mark progress? We also discuss how one might experimentally validate this approach in stages and what new research directions are opened by this paradigm.

\subsection{Integrating Inverse Decoding into Quantum Architectures}
Present-day quantum computing architectures, such as those pursued by IBM, Google (superconducting qubits), IonQ (trapped ions), or academic labs, predominantly use the circuit model with active QEC. To adopt inverse topological decoding, one approach is to build a specialized layer or module of the processor dedicated to protected qubits:
\begin{itemize}
    \item \textbf{Protected memory modules:} For example, in a superconducting quantum processor, one might allocate certain regions of the chip to act as stable memories using the Hamiltonian protection. These could store quantum information reliably while other parts of the chip perform logic gates using conventional methods. Periodically, the information could be swapped out of the protected modules to be processed, then re-encoded back for storage. This is analogous to how classical computers use error-corrected memory (ECC memory) that is separate from the CPU registers.
    \item \textbf{Full topological processor:} Alternatively, design the entire processor as a network of topologically protected qubits that can interact. This could mean something like a lattice of surface-code patches with tunable connections between patches for gate operations (e.g., lattice surgery operations can be mapped to merging/splitting surface code patches, which in hardware would mean temporarily connecting two patches via additional couplings). If each patch is self-correcting to some degree, performing lattice surgery would have to be done adiabatically or in steps so that errors during the operation are still suppressed or handled. This is a very ambitious design but aligns with a long-term vision of a ``topological quantum computer'' that was often imagined with anyons (like a whole system where computation is moves of anyons, not explicit gates).
    \item \textbf{Hybrid analog-digital schemes:} Another integration strategy is to keep the standard circuit approach but use analog error correction as a supplement. For example, run a normal QEC cycle but also have an always-on Hamiltonian that reduces error rates in between cycles. One must ensure the measurement and active steps don't conflict with the Hamiltonian (e.g., you might turn off or weaken certain couplings during measurements to avoid competition). After each cycle, the analog part handles residual or new errors until the next cycle. This hybrid could allow longer intervals between syndrome measurements, which reduces the bandwidth needed for classical processing.
\end{itemize}

For trapped ions, integration could mean a trap that has two modes: an analog stabilizing mode and a gate mode. Ions could be grouped into logical blocks where lasers are applied in a way to simulate the code Hamiltonian most of the time, and then reconfigured to perform gates via multi-qubit gate operations.

Majorana-based devices could naturally integrate as qubit storage; one can imagine a architecture where data is stored in Majorana qubits that are well-protected, and occasionally those are braided or coupled via intermediary circuits (like a transmon that couples to two topological qubits to enact a gate between them, which is a scheme some have considered with pi-junction or phase-controlled couplings).

A key part of integration is \textbf{interfacing analog-protected qubits with readout and control}:
We need to measure logical qubits or prepare them. If the qubit is encoded in a protected way, measuring it could be non-trivial (one might measure a parity of many physical qubits to collapse to logical $|0_L\rangle$ or $|1_L\rangle$). But if the hardware is already doing that, maybe measuring one qubit in the code is easier: e.g., measure all $Z$ stabilizers (which ideally are +1 in ground state) and then measure all physical qubits in Z basis, the product gives logical $Z$. Instead, perhaps coupling a readout resonator to a logical operator (like an interference or cat state trick) might read the logical qubit in one shot. For example, a special ancilla that couples to all qubits along a logical string can detect the logical parity. Designing such a readout is possible but complex.

Alternatively, one could decode at the end by gradually turning off the Hamiltonian and letting the state collapse to a simple form to read.

Another integration consideration: if the analog scheme fails rarely (like a logical error sneaks through once in a while), one could still have a background monitor. Perhaps have a slower classical decoding running in parallel to double-check. Or occasionally measure some stabilizers to be sure none have built-up errors. This is like a safety net.

\subsection{Experimental Validation Approaches}
Validating inverse topological decoding experimentally will likely proceed through incremental demonstrations:
\begin{enumerate}
    \item \textbf{Stabilizer Energy Gap:} Demonstrate that a certain multi-qubit configuration has the intended twofold degenerate ground state. This can be done by spectroscopy: prepare various states and measure energies, or do microwave spectroscopy to identify the gap between ground and first excited manifold. For example, create a 4-qubit plaquette in superconducting circuit and show that the $ZZZZ$ parity being +1 vs -1 has a measurable energy difference $\Delta$ because of the coupler.
    \item \textbf{Autonomous Anyon Annihilation:} Prepare a state with a known pair of syndrome excitations (e.g., in a surface code, flip one qubit to create two anyons) and then observe over time that the system returns to the no-anyon ground state without external action. This could be seen by measuring the stabilizers after some time to see if they self-corrected. Or by measuring the qubits to see if the error is gone.
    \item \textbf{Improved Qubit Lifetime:} The hallmark of error correction is extending the lifetime of quantum information. Show that an encoded qubit (with inverse decoding running) decays (loses fidelity) at a slower rate than any of the physical components. Initially, try to beat the best single qubit (break even). Then try to show an advantage that increases with code size (the start of a scaling trend).
    \item \textbf{Logical Operations Preservation:} Prepare a superposition in the logical qubit, let it evolve under protection for some time, and show that not only population is preserved, but coherence (phase information) is also preserved. This means the environment or analog process isn't dephasing the logical subspace. This is tricky because some types of noise (like fluctuating energies of logical states) could cause logical dephasing. But topological code symmetry should prevent splitting of the logical states at least to first order. If any splitting is present due to perturbations, one might refocus it via small corrections or additional symmetrization.
    \item \textbf{Small Algorithm or Gate:} Eventually, use a couple of these protected qubits to perform a short logical operation sequence, e.g., a logical CNOT or a small algorithm like logical teleportation, to see that the system can not only store but also process information without losing the benefit. This would be a step toward full fault-tolerant computation.
\end{enumerate}

Each step builds confidence. If any step fails to show improvement, analyze whether it's due to fundamental limits or just insufficient hardware performance.

For example, if we don't see lifetime improvement, maybe the environment (like 1/f noise) directly couples to the logical operator due to slight asymmetry, meaning we need a better symmetry or design. Or if excitations don't recombine, maybe our coupler was too static and needed a bit of drive to allow movement.

It will also be instructive to compare results to theory or simulation. Simulating even moderate size codes with a thermal bath is challenging but a few qubits can be done numerically. Experiments could calibrate such models.

\subsection{Future Research Directions}
The pursuit of hardware-embedded error correction in quantum computing opens several fertile research avenues:
\begin{itemize}
    \item \textbf{New Error-Correcting Phases of Matter:} We can ask, are there new phases or models that naturally perform error correction? Beyond the toric code, maybe certain \textit{fracton models} (where excitations cannot move freely) might localize errors in place until they pair with the exact partner. Fracton code models (like Haah's code) are being studied for stability; implementing them might yield better self-correction properties (Haah's code, for instance, is a 3D code that might have a higher energy barrier).
    \item \textbf{Continuous QEC Theory:} Develop the theoretical framework for continuous, Hamiltonian-based QEC. This overlaps with continuous quantum error correction (where error syndrome is monitored via weak measurements) but here in fully analog style. We might see connections to control theory (stability analysis of a quantum system with feedback).
    \item \textbf{Quantum Control and Machine Learning:} Using optimal control or even machine learning to design pulses or interactions that achieve the error correcting dynamics more efficiently. For example, a reinforcement learning agent might find a periodic sequence of Hamiltonian modulations that best dissipates errors.
    \item \textbf{Interdisciplinary synergy:} This area sits between quantum computing and quantum simulation. Techniques from one can benefit the other. For instance, methods to measure and characterize many-body states in AMO physics (like using quantum gas microscopy to detect anyons in a cold atom simulator of a code) could test these ideas in analog quantum simulators even before building an actual quantum computer out of it.
    \item \textbf{Fault-tolerant analog quantum computing:} If we succeed in building these protected qubits, what does the model of computation become? Perhaps a hybrid analog-digital approach yields a new paradigm which might simplify some aspects of algorithm design or compilation. Researchers might explore algorithms that inherently take advantage of an always-on error suppression (maybe modifying quantum circuits to assume qubits have a known parity coupling).
    \item \textbf{Assessing fundamental limits:} Investigate whether there are theoretical limits to how well a Hamiltonian can correct errors. For instance, no-go theorems in 2D say no finite temperature stability, but with non-equilibrium, is there an upper bound on the effective logical T2 one can get without measurements? Understanding these will tell us if inverse decoding can only delay errors but not completely eliminate the need for occasional measurements.
\end{itemize}

In conclusion, inverse topological decoding at the hardware level is a bold approach that unites ideas from quantum error correction, condensed matter physics, and quantum engineering. It demands a deep understanding of both abstract theory and real-world constraints. If successful, it could lead to quantum computers that are inherently more robust, pushing us closer to the dream of scalable, fault-tolerant quantum computation. The road is challenging, but the payoff is a more seamless integration of quantum error correction into the quantum hardware, much like error correction in classical systems is now an invisible, built-in aspect of memory and communication devices. 

\begin{thebibliography}{99}
\bibitem{Dennis2002} E.~Dennis, A.~Kitaev, A.~Landahl, and J.~Preskill (2002). \textit{Topological quantum memory}. J. Math. Phys. \textbf{43}(9), 4452–4505.
\bibitem{Fowler2012} A.~G.~Fowler, M.~Mariantoni, J.~M.~Martinis, and A.~N.~Cleland (2012). \textit{Surface codes: Towards practical large-scale quantum computation}. Phys. Rev. A \textbf{86}(3), 032324.
\bibitem{Gottesman1997} D.~Gottesman (1997). \textit{Stabilizer Codes and Quantum Error Correction}. Ph.D. thesis, Caltech. arXiv:quant-ph/9705052.
\bibitem{Kitaev2003} A.~Y.~Kitaev (2003). \textit{Fault-tolerant quantum computation by anyons}. Annals of Physics \textbf{303}(1), 2–30.
\bibitem{Bravyi2010} S.~Bravyi, M.~Hastings, and S.~Michalakis (2010). \textit{Topological quantum order: stability under local perturbations}. J. Math. Phys. \textbf{51}, 093512.
\bibitem{Pastawski2009} F.~Pastawski, A.~Kay, N.~Schuch, and I.~Cirac (2009). \textit{Limitations of passive quantum error correction}. Quantum Info. Comput. \textbf{10}, 580.
\bibitem{Kapit2016} E.~Kapit (2016). \textit{Hardware-efficient and fully autonomous quantum error correction in superconducting circuits}. Phys. Rev. Lett. \textbf{116}, 150501.
\bibitem{Terhal2015} B.~M.~Terhal (2015). \textit{Quantum error correction for quantum memories: a review}. Rev. Mod. Phys. \textbf{87}(2), 307–346.
\bibitem{Alicki2009} R.~Alicki, M.~Fannes, and M.~Horodecki (2009). \textit{On thermalization of topological quantum memories}. J. Phys. A: Math. Theor. \textbf{42}(6), 065303.
\bibitem{Nayak2008} C.~Nayak, S.~H.~Simon, A.~Stern, M.~Freedman, and S.~Das Sarma (2008). \textit{Non-Abelian anyons and topological quantum computation}. Rev. Mod. Phys. \textbf{80}(3), 1083–1159.
\bibitem{Baez2011} J.~C.~Baez (2011). \textit{An introduction to n-categories}. In E.~Davies and R.~Hajar, eds., \textit{7th Conference on Category Theory and Computer Science}.
\bibitem{Walker2012} K.~Walker and Z.~Wang (2012). \textit{Universal quantum gates in 2D qubit topological quantum computers}. Front. Phys. \textbf{7}, 150.
\bibitem{Bombin2007} H.~Bombin and M.~A.~Martin-Delgado (2007). \textit{Homological error correction: Classical and quantum codes}. J. Math. Phys. \textbf{48}(5), 052105.
\bibitem{Acharya2023} R.~Acharya \textit{et al.} (Google Quantum AI) (2023). \textit{Suppressing quantum errors by scaling a surface code logical qubit}. Nature \textbf{614}, 676–681.
\bibitem{Gladchenko2009} S.~Gladchenko \textit{et al.} (2009). \textit{Superconducting quantum circuit implementing a Stern–Gerlach spin filter}. Nature Physics \textbf{5}, 48–53.
\bibitem{Xu2022Coupler} Y.~Xu \textit{et al.} (2022). \textit{Realizing all-to-all coupling in a superconducting quantum processor via a multi-path coupler}. arXiv:2203.17241.
\bibitem{Lescanne2020} R.~Lescanne \textit{et al.} (2020). \textit{Exponential suppression of bit-flips in a qubit encoded in an oscillator}. Nature Phys. \textbf{16}, 509–513.
\bibitem{Alicea2012} J.~Alicea (2012). \textit{New directions in the pursuit of Majorana fermions in solid state systems}. Rep. Prog. Phys. \textbf{75}(7), 076501.
\bibitem{Terhal2012MajoranaSurf} B.~M.~Terhal, F.~Hassler, and D.~P.~DiVincenzo (2012). \textit{From Majorana fermions to topological order}. Phys. Rev. Lett. \textbf{108}(26), 260504.
\bibitem{Vijay2015} S.~Vijay, T.~H.~Hsieh, and L.~Fu (2015). \textit{Majorana fermion surface code for universal quantum computation}. Phys. Rev. X \textbf{5}, 041038.
\bibitem{Karzig2017} T.~Karzig \textit{et al.} (2017). \textit{Scalable designs for quasiparticle-poisoning-protected topological quantum computation with Majorana zero modes}. Phys. Rev. B \textbf{95}, 235305.
\bibitem{RyanAnderson2021} C.~Ryan-Anderson \textit{et al.} (2021). \textit{Realization of real-time fault-tolerant quantum error correction}. Phys. Rev. X \textbf{11}, 041058.
\bibitem{Monroe2019} J.~Choi \textit{et al.} (2019). \textit{Exploring the many-body localization transition in two dimensions}. Science \textbf{364}(6437), 256–260. [\textit{(Uses trapped ion simulator for 2D spin models)}].
\bibitem{Negnevitsky2018} V.~Negnevitsky \textit{et al.} (2018). \textit{Repeated multi-qubit readout and feedback with a mixed-species trapped-ion register}. Nature \textbf{563}, 527–531.
\bibitem{Homeier2023Ry} L.~E.~Homeier \textit{et al.} (2023). \textit{Z2 lattice gauge theories on a quantum computer}. PRX Quantum \textbf{4}, 020329.
\bibitem{Leghtas2013} Z.~Leghtas \textit{et al.} (2013). \textit{Hardware-efficient autonomous quantum memory protection}. Phys. Rev. Lett. \textbf{111}, 120501.
\bibitem{Ofek2016} N.~Ofek \textit{et al.} (2016). \textit{Extending the lifetime of a quantum bit with error correction in superconducting circuits}. Nature \textbf{536}, 441–445.
\bibitem{Mourik2012} V.~Mourik \textit{et al.} (2012). \textit{Signatures of Majorana fermions in hybrid superconductor-semiconductor nanowire devices}. Science \textbf{336}(6084), 1003–1007.
\bibitem{Lu2017NMR} D.~Lu \textit{et al.} (2017). \textit{Experimental estimation of average fidelity of a clifford gate on a 7-qubit quantum processor}. Phys. Rev. Lett. \textbf{119}, 150502. [\textit{Includes NMR simulation of small codes}].
\bibitem{Higgott2022} O.~Higgott and S.~Brierley (2022). \textit{Pymatching: A Python package for decoding quantum codes with minimum-weight perfect matching}. Quantum \textbf{6}, 765.
\end{thebibliography}

\end{document}
